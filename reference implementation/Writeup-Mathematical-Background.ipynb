{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Overview\n",
    "\n",
    "In this section we present a mathematical overview of using Linear Predictive Coding (LPC) to extract spectral envelopes for cross-synthesis.\n",
    "\n",
    "Before explaining LPC, we must first cover some necessary background: the mathematical representations of filters, and the spectral analysis of noise. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filters \n",
    "\n",
    "A *Linear Time Invariant (LTI)* filter can be represented in the time-domain as an impulse-response function $h$ that convolves an input signal $x$ to produce a filtered output signal $y$.\n",
    "\n",
    "$$\n",
    "y(n) = (h * x)(n)\n",
    "$$\n",
    "\n",
    "By the Convolution Theorem this corresponds to windowing in the frequency domain, where $H(k)$ is called the *transfer function* of the filter:\n",
    "\n",
    "$$\n",
    "Y(k) = H(k)X(k) \\implies H(k) = \\frac{Y(k)}{X(k)}\n",
    "$$\n",
    "\n",
    "An alternative model represents filters as computes an output signal $y(n)$ from a linear combination of past input and output samples:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y(n) &= b_0x(n) + b_1x(n-1) + ... + b_Mx(n-N) \\\\ \n",
    "     & \\          - a_1y(n-1) - ... - a_Ny(n-M) \\\\\n",
    "     &= \\sum_{i=0}^{N}b_ix(n-i) - \\sum_{j=1}^{M}a_jy(n-j)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This representation is called the *Difference Equation* of the filter, with ${b}_{i=0}^N$ feedfoward coefficients and ${a}_{j=1}^M$ feedback coefficients. \n",
    "\n",
    "__Note: $i$ and $j$ here function as indices, not as the complex value $\\sqrt{-1}$__!\n",
    "\n",
    "Let $\\omega = e^{-\\sqrt{-1}(2\\pi)/N}$. Taking the DFT of the difference equation, we find\n",
    "\n",
    "\\begin{align}\n",
    "Y(k) &= DFT[\\sum_{i=0}^{N}b_ix(n-i) - \\sum_{j=1}^{M}a_jy(n-j)] \\\\\n",
    "&= DFT[\\sum_{i=0}^{N}b_ix(n-i)] - DFT[\\sum_{j=1}^{M}a_jy(n-j)] \\\\\n",
    "&= \\sum_{i=0}^{N}b_iDFT[SHIFT_i\\{x(n)\\}] - \\sum_{j=1}^{M}a_jDFT[SHIFT_j\\{y(n)\\}] \\\\\n",
    "&= \\sum_{i=0}^{N}b_i\\omega^iX(k) - \\sum_{j=1}^{M}a_j\\omega^jY(k) \\\\\n",
    "&= X(k)\\underbrace{\\sum_{i=0}^{N}b_i\\omega^i}_{DFT[b]} - Y(k)\\sum_{j=1}^{M}a_j\\omega^j \\\\\n",
    "\\end{align}\n",
    "\n",
    "Rearranging, we have\n",
    "\n",
    "\\begin{aligned}\n",
    "& Y(k)\\underbrace{[1 + \\sum_{j=1}^{M}a_j\\omega^j]}_{DFT[a]} = X(k)\\underbrace{\\sum_{i=0}^{N}b_i\\omega^i}_{DFT[b]} \\\\\n",
    "& \\implies Y(k)A(k) = X(k)B(k) \\\\\n",
    "& \\implies H(k) = \\frac{Y(k)}{X(k)} = \\frac{B(k)}{A(k)} \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Hence the transfer function $H$ of an LTI filter may also be represented as the quotient of its feedfoward and feedback coefficients in the frequency domain. This representation will be relevant to understanding LPC as computing a recursive, all-pole filter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrum Analysis of Noise\n",
    "\n",
    "A statistical interpretation of LPC assumes all time-domain signals can be modeled as outputs from an all-pole filter driven by white noise. (Note all-pole means the difference equation of the filter has 0 feedfoward coefficients, and only feedback coefficients). \n",
    "\n",
    "In statistical signal processing, *white noise* is modeled as a *stationary stochastic process*, i.e. a sequence of random variables all drawn from the same, static probability distribution. Given such a noise sequence $x \\in \\mathbb{R}^N$, we assume the distribution has $\\mu_x = 0$ and variance $\\sigma_x^2$. \n",
    "\n",
    "What are the spectral characteristics of this noise sequence? It will help to recall the definition of the *Power Spectral Density* function\n",
    "\n",
    "$$\n",
    "S_x(k) = DFT_k[r_x]\n",
    "$$\n",
    "\n",
    "Where $r_x$ is the *autocorrelation function*\n",
    "\n",
    "$$\n",
    "r_x(m) = \\frac{1}{N}(x \\otimes x)(m) = \\frac{1}{N}\\sum_{n=0}^{N-1}x(n)x(n+m)\n",
    "$$\n",
    "\n",
    "For a white-noise signal $x$ with $\\mu_x = 0$, note that as $N \\to \\infty$, \n",
    "\n",
    "$$\n",
    "r_x(0) = \\frac{1}{N}\\sum_{n=0}^{N-1}|x(n)|^2 = \\sigma_x^2\n",
    "$$\n",
    "\n",
    "And for $m \\neq 0$, the expected value $E[r_x(m)] \\to 0$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[r_x(m)] &= E[\\frac{1}{N}\\sum_{n=0}^{N-1}x(n)x(n+m)] \\\\\n",
    "&= \\frac{1}{N}\\sum_{n=0}^{N-1}E[x(n)]E[x(n+m)] \\\\\n",
    "&= \\frac{1}{N}\\sum_{n=0}^{N-1}\\mu_x^2 \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The autocorrelation function $r_x(m)$ of a white noise signal $x(n)$ is therefore all 0s except at index 0 where $r_x(0) = \\sigma_x^2$. In otherwords, the autocorrelation of white noise (averaged over infinite samples) is equivalent to the weighted impulse function $r_x = \\sigma_x^2\\delta$.\n",
    "\n",
    "The Power Spectral Density of white noise is therefore proportional to the DFT of the impulse function, which gives a uniform frequency response across all frequency bins.\n",
    "\n",
    "$$\n",
    "S_x(k) = DFT_{k}[r_x] = DFT_k[\\sigma_x^2\\delta] = \\sigma_x^2\n",
    "$$\n",
    "\n",
    "Hence we say the average magnitude spectrum of white noise is *flat*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Prediction Coding\n",
    "\n",
    "Now we are ready to analyze LPC. Given an input signal $y \\in \\mathbb{R}^N$, linear prediction aims to find a set of *prediction coefficients* $\\{a\\}_{i=1}^M$ such that $\\forall n < N, y(n)$ can be accurately estimated as a linear combination of the previous $M$ samples. I.e.\n",
    "\n",
    "$$\n",
    "y(n) = e(n) - \\sum_{i=1}^{M}a_iy(n-i)\n",
    "$$\n",
    "\n",
    "Here $M$ denotes the *order* of the linear predictor, and $e(n)$ denotes the *prediction error* at sample $n$. Computing this linear predictor requires solving an optimization problem: what are the optimal $\\{a\\}_{i=1}^M$ which minimize the sum of squared errors $||e||^2$? \n",
    "\n",
    "Before delving into a solution, note how similar the above linear equation looks to the difference equations discussed earlier! Like we did with difference equations, let's take the DFT of both sides:\n",
    "\n",
    "\\begin{align}\n",
    "Y(k) &= DFT[e] - DFT[\\sum_{i=1}^{M}a_iy(n-i)] \\\\\n",
    "&= E(k) - \\sum_{i=1}^{M}a_iDFT[SHIFT_i\\{y(n)\\}] \\\\\n",
    "&= E(k) - Y(k)\\sum_{i=1}^{M}a_i\\omega^i \n",
    "\\end{align}\n",
    "\n",
    "Rearranging terms and factoring out $Y(k)$, we have\n",
    "\n",
    "$$\n",
    "Y(k)\\underbrace{[1 + \\sum_{i=1}^{M}a_i\\omega^i]}_{A(k)} = E(k) \\implies Y(k) = \\frac{E(k)}{A(k)} \\implies \\frac{1}{A(k)} = \\frac{Y(k)}{E(k)}\n",
    "$$\n",
    "\n",
    "This last equation provides remarkable insight into what exactly LPC is doing, and how it is managing to extract the spectral envelope of the input signal $y$. The prediction coefficients found by LPC correspond to a designing a filter with impulse response $h = \\{1, a_1, a_2, ..., a_m\\}$ and transfer function $H(k) = 1/A(k)$ that maps the error spectrum $E(k)$ to the signal spectrum $Y(k)$. Minimizing $||e||^2$ corresponds to maximally flattening $||E||$, and as discussed above, flat spectra indicate the underlying signal can be modeled as white noise. Once $||E||$ has been optimized, $h$ will function as a noise-driven recursive filter that approximately reconstructs the original signal $y$! And because $|E(k)|$ is flat for white noise, the filter's frequency response $|A(k)|$ must also approximate $|Y(k)|$. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& (h * e) \\approx y & H(k)E(k) \\approx Y(k) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "It makes sense that $e(n)$ can be modeled as white noise under optimal prediction coefficients because it represents all new, unpredictable information entering the signal at sample $n$. Furthermore, if $||e||^2$ is minimized, all $e(i)$ should be independent, with no correlation to one another. Let us therefore assume the prediction error can be modeled as white noise with $\\mu_e = 0$ and variance $\\sigma_e^2$.\n",
    "\n",
    "Recalling that by the Correlation theorem, $y \\otimes y \\longleftrightarrow |Y(k)|^2$ we can now formally define the spectral envelope of $y$:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "S_y(k) &= DFT[\\frac{1}{N}(y \\otimes y)] \\\\\n",
    "& = \\frac{1}{N}|Y(k)|^2 = \\frac{|E(k)|^2/N}{|A(k)|^2} \\\\\n",
    "&= \\frac{S_e(k)}{|A(k)|^2} = \\frac{\\sigma_e^2}{|A(k)|^2} \\\\\n",
    "\\text{EnvelopeLPC}_y(k) &= \\sqrt{S_y(k)} = \\frac{\\sigma_e}{|A(k)|}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In words, the spectral envelop found by LPC is inverse magnitude spectrum of the prediction filter $A(k)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving for Prediction Coefficients\n",
    "\n",
    "Let $\\{\\hat{a}\\}_i^M$ denote the optimal prediction coefficients which minimize $||e||^2$. We can find these values by taking the partial derivative of the *cost function* $J(a) : \\mathbb{R}^M \\to \\mathbb{R}$ with respsect to each $a_i$, and setting it equal to 0. This 0-point represents a minimum in the cost function along the space $a_i=\\hat{a_i}$. Solving this set of equations corresponds to finding the values of each $a_i$ which minimize $||e||^2$ \n",
    "\n",
    "\\begin{align}\n",
    "J(a) &= ||e^2|| = \\sum_{n=0}^{N-1}[y(n) + \\sum_{i=1}^{M}a_iy(n-i)]^2 \\\\\n",
    "\\frac{\\partial J}{\\partial a_k} &= 2\\sum_{n=0}^{N-1}[y(n) + \\sum_{i=1}^{M}a_iy(n-i)]y(n-k) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Rearranging terms to solve for $\\partial J / \\partial a_k = 0$, we have\n",
    "\n",
    "\\begin{align}\n",
    "-\\sum_{n=0}^{N-1}y(n)y(n-k) &= \\sum_{n=0}^{N-1}\\sum_{i=1}^{M}\\hat{a_i}y(n-i)y(n-k) \\\\\n",
    "&= \\sum_{i=1}^{M}\\hat{a_i}\\sum_{n=0}^{N-1}y(n-i)y(n-k)\n",
    "\\end{align}\n",
    "\n",
    "The above equality can now be expressed in terms of the Bartlett-windowed biased acyclic autocorrelation function $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\gamma_y(k) = \\sum_{n}y(n)y(n+k)\n",
    "$$\n",
    "\n",
    "Note that for real signals this autocorrelation is a real, even function, i.e. $\\gamma(k) =  \\gamma(-k) \\in \\mathbb{R}$ Substituting, we now have\n",
    "\n",
    "$$\n",
    "-\\gamma_y(k) = \\sum_{i=1}^M\\hat{a_i}\\gamma_y(|k-i|)\n",
    "$$\n",
    "\n",
    "Because we are taking the partial derivative of $J(a)$ with respect to each $a_k$, we end up with a system of $M$ linear equations and $M$ unknowns, which may be represented as a matrix multiplication of matrix $R \\in \\mathbb{R}^{M \\times M}$ and $p \\in \\mathbb{R}^M$:\n",
    "\n",
    "$$\n",
    "R\\hat{a}= p\n",
    "$$\n",
    "\n",
    "Here $R(i,k) = \\gamma_y(|k-i|)$ and $p(k) = -\\gamma_y(k)$. The optimal prediction coefficients $\\hat{a}$ can then be found by:\n",
    "\n",
    "$$\n",
    "\\hat{a} = R^{-1}p\n",
    "$$\n",
    "\n",
    "Note that the covariance matrix $R$ is symmetric and *Toeplitz*, and therefore there exists an $O(M^2)$ solution to the above system of linear equations via the *Levinson-Durbin Algorithm*. Dicussing this algorithm goes beyond the scope of our project. In practice, we often have very small $M$, e.g. $M<10$ when modeling human speech, and so using a built-in linear equation solver such as `np.linalg.lstsq` is perfectly functional.\n",
    "\n",
    "Lastly, we can extract the spectral envelope of $y$ by computing:\n",
    "\n",
    "$$\n",
    "\\text{EnvelopeLPC}_y(k) = \\frac{1}{|DFT_k(\\hat{a})|} = \\frac{1}{|\\hat{A}(k)|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why LPC envelopes are \"good\"\n",
    "\n",
    "We can give some brief mathematical intuition behind why LPC is a \"good\" envelop extractor as follows. Let $\\hat{Y}$ denote our *estimated* signal output and $\\hat{A}$ our estimated prediction coefficients. By Rayleigh's Energy theorem,\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{n=1}^N|e(n)|^2 &= \\frac{1}{N}\\sum_{k=1}^N|E(k)|^2 \\\\\n",
    "&= \\frac{1}{N}\\sum_{k=1}^N|A(k)Y(k)|^2 \\\\\n",
    "&= \\frac{1}{N}\\sum_{k=1}^N|\\frac{\\sigma_e^2}{\\hat{Y}(k)}Y(k)|^2 \\\\\n",
    "&= \\frac{\\sigma_e^2}{N}\\sum_{k=1}^N|\\frac{Y(k)}{\\hat{Y}(k)}|^2 \\\\\n",
    "\\end{align}\n",
    "\n",
    "We can see that $|e(n)|^2$ is minimized when $\\hat{Y}(k) > Y(k)$ i.e. when the estimated spectral magnitude exceeds the signal spectral magnitude. This means that LPC prioritizes tracking peaks over valleys, making for more accurate envelope extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://ccrma.stanford.edu/~jos/sasp/Spectrum_Analysis_Noise.html\n",
    "\n",
    "https://ccrma.stanford.edu/~jos/sasp/Cross_Synthesis.html\n",
    "\n",
    "https://ccrma.stanford.edu/~jos/filters/Z_Transform_Difference_Equations.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
